# -*- coding: utf-8 -*-
"""Q3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FC0vpdxgBak55VGJrhBMblvDRZrqi_Zu

# **Dataset Generation and Split**

---
"""

import numpy as np

# Generate synthetic data
n_per_class = 10
X0 = np.random.multivariate_normal(mean=[-1, -1], cov=np.eye(2), size=n_per_class)
y0 = np.zeros(n_per_class)
X1 = np.random.multivariate_normal(mean=[1,  1], cov=np.eye(2), size=n_per_class)
y1 = np.ones(n_per_class)

# Combine and shuffle
X = np.vstack([X0, X1])
y = np.concatenate([y0, y1])
perm = np.random.permutation(len(y))
X, y = X[perm], y[perm]

# 50/50 train-test split
split = len(y) // 2
X_train, y_train = X[:split], y[:split]
X_test,  y_test  = X[split:], y[split:]

print(f"Train samples: {X_train.shape[0]}, Test samples: {X_test.shape[0]}")

"""# **Parameter Initialization**

---


"""

# Initialize weights and biases randomly
W1 = np.random.randn(1, 2)   # (1 hidden neuron × 2 inputs)
b1 = np.random.randn()       # scalar bias for hidden neuron
W2 = np.random.randn()       # scalar weight from hidden to output
b2 = np.random.randn()       # scalar bias for output

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

"""# **Training with Gradient Descent and Squared Error Loss**

---


"""

lr = 0.1               # learning rate
epochs = 1000          # number of iterations
N_train = X_train.shape[0]

for epoch in range(1, epochs + 1):
    # Forward pass
    Z1 = X_train.dot(W1.T).ravel() + b1          # shape (N_train,)
    A1 = sigmoid(Z1)                             # hidden activations
    Y_pred = W2 * A1 + b2                        # linear output

    # Compute training loss (MSE)
    error = y_train - Y_pred
    loss = np.mean(error**2)

    # Backward pass (gradients)
    dY_pred = -2 * error / N_train               # dL/dY_pred
    dW2 = np.dot(dY_pred, A1)                    # gradient wrt W2
    db2 = np.sum(dY_pred)                        # gradient wrt b2

    # backprop into hidden layer
    dZ1 = dY_pred * W2 * (A1 * (1 - A1))         # shape (N_train,)
    dW1 = dZ1.dot(X_train)                       # gradient wrt W1 (shape (2,))
    db1 = np.sum(dZ1)                            # gradient wrt b1

    # Gradient descent updates
    W2 -= lr * dW2
    b2 -= lr * db2
    W1 -= lr * dW1.reshape(W1.shape)             # reshape to (1,2)
    b1 -= lr * db1

    if epoch % 100 == 0:
        print(f"Epoch {epoch:04d} — Training MSE: {loss:.4f}")

"""# **Evaluate on Test Set**

---


"""

# Forward pass on test data
Z1_test = X_test.dot(W1.T).ravel() + b1
A1_test = sigmoid(Z1_test)
Y_test_pred = W2 * A1_test + b2

# Compute test MSE
test_mse = np.mean((y_test - Y_test_pred)**2)
print(f"\nTest MSE: {test_mse:.4f}")